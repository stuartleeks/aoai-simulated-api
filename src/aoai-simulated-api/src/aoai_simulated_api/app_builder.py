import logging
import traceback
from typing import Annotated
from fastapi import Depends, FastAPI, Request, Response, HTTPException

from aoai_simulated_api.auth import validate_api_key_header
from aoai_simulated_api.config_loader import get_config, set_config
from aoai_simulated_api.generator.manager import invoke_generators
from aoai_simulated_api.latency import LatencyGenerator
from aoai_simulated_api.limiters import apply_limits
from aoai_simulated_api.models import RequestContext
from aoai_simulated_api.record_replay.handler import RecordReplayHandler
from aoai_simulated_api.record_replay.persistence import YamlRecordingPersister


logger = logging.getLogger(__name__)

app = FastAPI()


# pylint: disable-next=invalid-name
record_replay_handler = None


def apply_config():
    # pylint: disable-next=global-statement
    global record_replay_handler

    record_replay_handler = None

    logger.info("üöÄ Starting aoai-simulated-api in %s mode", get_config().simulator_mode)
    logger.info("üóùÔ∏è Simulator api-key                       : %s", get_config().simulator_api_key)

    if get_config().simulator_mode in ["record", "replay"]:
        logger.info("üìº Recording directory                     : %s", get_config().recording.dir)
        logger.info("üìº Recording auto-save                     : %s", get_config().recording.autosave)
        persister = YamlRecordingPersister(get_config().recording.dir)

        record_replay_handler = RecordReplayHandler(
            simulator_mode=get_config().simulator_mode,
            persister=persister,
            forwarders=get_config().recording.forwarders,
            autosave=get_config().recording.autosave,
        )
    else:
        logger.info("üìù allow_undefined_openai_deployments      : %s", get_config().allow_undefined_openai_deployments)

    logger.info("üìù Using OpenAI deployments                : %s", get_config().openai_deployments)
    logger.info("üìù Using latencies                         : %s", get_config().latency)
    logger.info("üìù Using limits_storage_connection_string  : %s", get_config().limits_storage_connection_string)


def _default_validate_api_key_header(request: Request):
    validate_api_key_header(request=request, header_name="api-key", allowed_key_value=get_config().simulator_api_key)


@app.get("/")
async def root():
    return {"message": "üëã aoai-simulated-api is running"}


@app.post("/++/save-recordings")
def save_recordings(_: Annotated[bool, Depends(_default_validate_api_key_header)]):
    if get_config().simulator_mode == "record":
        logger.info("üìº Saving recordings...")
        record_replay_handler.save_recordings()
        logger.info("üìº Recordings saved")
        return Response(content="üìº Recordings saved", status_code=200)

    logger.warning("‚ö†Ô∏è Not saving recordings as not in record mode")
    return Response(content="‚ö†Ô∏è Not saving recordings as not in record mode", status_code=400)


@app.get("/++/config")
def config_get(_: Annotated[bool, Depends(_default_validate_api_key_header)]):
    # return a subset of the config as not all properties make sense (e.g. generator functions)
    config = get_config()
    return {
        "simulator_mode": config.simulator_mode,
        "latency": {
            "open_ai_embeddings": {
                "mean": config.latency.open_ai_embeddings.mean,
                "std_dev": config.latency.open_ai_embeddings.std_dev,
            },
            "open_ai_completions": {
                "mean": config.latency.open_ai_completions.mean,
                "std_dev": config.latency.open_ai_completions.std_dev,
            },
            "open_ai_chat_completions": {
                "mean": config.latency.open_ai_chat_completions.mean,
                "std_dev": config.latency.open_ai_chat_completions.std_dev,
            },
        },
        "openai_deployments": (
            {
                name: {"tokens_per_minute": deployment.tokens_per_minute, "model": deployment.model}
                for name, deployment in config.openai_deployments.items()
            }
            if config.openai_deployments
            else None
        ),
    }


@app.patch("/++/config")
def config_patch(config: dict, _: Annotated[bool, Depends(_default_validate_api_key_header)]):
    original_config = get_config()

    # Config is a nested settings class to enable setting env var names on child items
    # As a result we need to update each level independently
    root_dict = {k: v for k, v in config.items() if k in ["simulator_mode", "allow_undefined_openai_deployments"]}
    new_config = original_config.model_copy(update=root_dict)
    if "latency" in config:
        if "open_ai_completions" in config["latency"]:
            new_config.latency.open_ai_completions = original_config.latency.open_ai_completions.model_copy(
                update=config["latency"]["open_ai_completions"]
            )
        if "open_ai_chat_completions" in config["latency"]:
            new_config.latency.open_ai_chat_completions = original_config.latency.open_ai_chat_completions.model_copy(
                update=config["latency"]["open_ai_chat_completions"]
            )
        if "open_ai_embeddings" in config["latency"]:
            new_config.latency.open_ai_embeddings = original_config.latency.open_ai_embeddings.model_copy(
                update=config["latency"]["open_ai_embeddings"]
            )

    # Update the config and re-initialize
    set_config(new_config)
    apply_config()

    return config_get(_)


@app.api_route("/{full_path:path}", methods=["GET", "POST", "PUT", "DELETE"])
async def catchall(request: Request):
    logger.debug("‚ö° handling route: %s", request.url.path)

    response = None
    context = RequestContext(config=get_config(), request=request)

    try:
        # LatencyGenerator adds simulated latency to response
        # and emit associated metrics
        async with LatencyGenerator(context) as latency_generator:
            # Get response
            if get_config().simulator_mode == "generate":
                response = await invoke_generators(context, get_config().generators)
            elif get_config().simulator_mode in ["record", "replay"]:
                response = await record_replay_handler.handle_request(context)

            if not response:
                logger.error("No response found for request: %s", request.url.path)
                return Response(status_code=500)

            # Apply limits here so that that they apply to record/replay as well as generate
            if response.status_code < 300:
                response = await apply_limits(context, response)

            # pass the response to the latency generator
            # so that it can determine the latency to add
            latency_generator.set_response(response)

            return response
    except HTTPException as he:
        raise he
    # pylint: disable-next=broad-exception-caught
    except Exception as e:
        logger.error("Error: %s\n%s", e, traceback.format_exc())
        return Response(status_code=500)
